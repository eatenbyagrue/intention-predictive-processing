# Biblatex
#+LATEX_HEADER: \usepackage[backend=biber,authordate, ibidtracker=context,natbib,doi=false,isbn=false,url=false]{biblatex-chicago}
#+LATEX_HEADER: \addbibresource{~/Documents/bibliography/references.bib}

# Line spacing
#+LATEX_HEADER: \usepackage{setspace}
#+LATEX_HEADER: \onehalfspacing

# Abbreviations
#+LATEX_HEADER: \newcommand{\Su}[1]{\citep[p.~#1]{clark16_surfin_uncer}} 
# Title etc.
#+LATEX_HEADER: \author{Conrad Friedrich}
#+OPTIONS: toc:nil num:t author:nil subtitle:nil
#+TITLE: Intentional Action and Predictive Processing
#+SUBTITLE: XxXx words

#+begin_abstract 
This paper addresses the question whether the concept of intention as in intentional action used in philosophy and general folk psychological discourse has an apt correspondence in quite recent unifying theories of perception, action, and cognition in the cognitive sciences grouped under the label predictive processing. I argue that there are very prominent junctions of both seemingly separate discussions which merit fruitful research, but identify and highlight obstacles along the way. In order to bring both discussion on a common denominator, I first survey the philosophical literature to pick out relevant arguments and positions. Then I describe the predictive approach to cognitive science as received in the recent philosophical and empirical literature. Following that, I highlight where both approaches treat on common ground and give an optimistic perspective, while keeping in mind the separate aim of both fields. 
#+end_abstract


\thispagestyle{empty}
\newpage

* Introduction

What is intention? When is an action intentional? The philosophical literature on this question is quite broad, not just since Anscombe's /Intention/ in 1957. Philosophers of action found plenty to discuss by asking themselves what we mean when we say that an action was intentional. In this paper, I'll first (Section [[Intentions in Philosophy]]) take the philosophical discussion and distill the core relevant arguments. Second (Section [[Predictive Processing]]) II'll discuss a framework in cognitive science recently composed by Andy Clark dubbed predictive processing. A central premise of this framework is to negate the conventional picture of perception as passive processing of sensory input, incrementally building a more and more complex percepts out of raw building blocks. Instead, the framework inverts this view by postulating that a cognitive system proactively and constantly predicts its own sensory input based on few central principles. This framework is used to develop models for diverse cognitive phenomena and purports to give a unifying underlying explanation. The clou---and relevance---for the present paper comes in its aspiration to also account for many cognitive aspect relevant to /action/. I ask, then (Section [[Intentions in Predictive Processing]]), in what sense, if any, can the philosophical concept of intention and intentional action be found and reconstructed in the predictive processing framework? I find that there certainly are characteristic aspects and arguments in the philosophical literature that can be usefully adapted to make sense in the cognitive framework. But, perhaps unsurprisingly, there are presuppositions involved which make some aspects almost incommensurable. 

During the writing, I tried my best to avoid the technical jargon I noticed is ubiquitous in both fields surveyed. But since the issues are hardly separable from the descriptions used this was not an easy task to keep up. In any case, I hope the relations between both fields were saliently highlighted.

Maybe a word about methodology is in order. On first glance, it might seem strange to try and connect two fields which have more or less clearly stated separate aims. On the on hand, the philosophical literature following Elizabeth Anscombe was still heavily inspired by Wittgenstein and Austin and viewed the analysis of natural language as a primary method of philosophical investigation. On the other hand, the much more recent development of frameworks in the philosophy of cognitive science are unsurprisingly much more empirically minded. I agree that there are clear differences in their aims and methods, but isn't it still the case that its highly relevant for conceptual analysis what actually is the case, rather than only how we use a concept? And conversely, can't a clear account of intricacies of natural language use and conceptual intuition inform at least some models in cognitive science? For me, this is clearly the case, as long as one keeps in mind and is explicit about this further complication. 

With this out of the way, I scavenge the philosophical literature for useful bits to guide the discussion of intention in the following section. 

* Intentions in Philosophy

Intentions are intuitively meaningful ascriptions of a person, which we naturally use in everyday conversation. Intentions signify the difference of myself /doing/ something or that thing merely /happening/ to me. Things get a bit more complicated, though, once we try to pin down the exact boundaries and cases of the concept. Let's look briefly at some of the distinctions, problems and arguments . 

There are two central questions in the discussions relevant here:
1. What is the relation of intention and action?
2. What is the relation of intention and mental states, like propositional attitudes?

As to question (1), Elizabeth citet:anscombe57_inten influentially offered to disambiguate the relation of intention and action into three different use cases in normal, pre-theoretic discourse: Cases where (i) an agent intends to \phi, where (ii) the agent \(\phi\)s intentionally, and where (iii) the agent \(\psi\) with the intention of \phi-ing citep:wilson16_action, where \phi and \psi stand in for action descriptions. In case (i), an agent might merely intend to \phi, but never come around and actually do it, whether she decides otherwise or something comes up that prohibits her from doing so. For example, I intend to finish this paper on time. In case (ii), \phi-ing is what she does /and/ what she is intending. For example, I'm typing these words intentionally. The object of her intention and her current action coincide. In case (iii), these two come apart: She \(\psi\)s with the further intention to \phi. For example, I'm writing these words with the intention to write a paper (Examples borrowed from citet:sep-intention). For Anscombe, a successful analysis of the concept of intention should account for all three usages. It is noteworthy that in each case, intention is directed at the future. The degree to which this is the case can differ markedly, as e.g. in case (ii), which is directed at something like the rolling present or the very near future. 

It is quite obvious already, then, that intentions cannot be completely reduced to action, since there are plausible cases of intentions without actions. 

Now, what is it to act intentionally? What does that mean? 

A natural answer is that intentions /cause/ their intended action. They give a reason as to why the agent acted that way. This /causalist/ position is argued for by, e.g., citet:davidson63_action_reason_causes. citet:anscombe57_inten is the canonical representative of an /anti/-causalist position which essentially denies a causally relation between intention and action. The causalist gives a straightforward, intuitive picture of how intention and action might relate, but is open to important objections, notably: Causal chains can be devious. Even though I intend an action, my intention involuntarily might cause another action, which then causes me to act as originally intended. For example, ``A climber might want to rid himself of the weight and danger of holding another man on a rope, and he might know that by loosening his hold on the rope he could rid himself of the weight and danger. This belief and want might so unnerve him as to cause him to loosen his hold, and yet it might be the case that he never chose to loosen his hold, nor did he do it intentionally.'' citep:bonicalzi_agenc_mental_repres. Here, there is a direct causal chain from what reasonably might be described as an intention on part of the climber to the intended action, although the action was not intentional. The action is not connected to the intention in /the right way/. 

To address this problem, citet:searle83_inten distinguishes prospective intention and intention in action. A prospective intention precedes an intentional action, whereby an intention in action occurs simultaneous with the action. Both appear in a planned intentional action, while only the latter is present in a spontaneous action. But how does this counter the case of the deviant causal chain? Searle requires the intention in action to be present throughout the bodily movement that constitutes the action. The intention is a supporting cause throughout the movement. If this is not the case, then the action cannot be counted as intentional. And this is exactly what happens when the climber is so unnerved as to loosen his grip, which is not supported throughout by an intention. 

As to the question (2) regarding the relation of intention and mental state, an important distinction in the literature is whether to identify or, at least, intimately relate ascriptions of intentions with ascriptions of certain mental states. For the moment, the concern is just with propositional attitudes as mental states, although there are, of course, other mental states. Propositional attitudes describe states of an agent which are directed towards a proposition, expressed by sentences of the form `She believes that /p/', where /p/ stands for a proposition. [[citet:anscombe57_inten][\S 32]] gives an important difference between propositional attitudes: They might have a mind-to-world or world-to-mind /direction of fit/. Beliefs, as they tend to represent the world, fit their representation to the world. Desires strive to fit the world according to their content. Intentions, conceived as mental states, apparently fall into the second category, they seem to have world-to-mind direction of fit. But not trivially so, as the case of Oedipus makes salient. Oedipus intended to marry Jocasta and not his mother. Yet the difference between both descriptions is his ignorance: Just a simple belief missing, and beliefs have mind-to-world direction of fit. 

But intending to \phi implies more than just desiring to \phi: There is a certain level of commitment evolved, deliberation on whether to \phi is over, as citet:Bratman1987-BRAIPA describes it.

The position that intentions consist in or are determined by propositional attitudes can be addressed as /cognitivism/ expressing that intentions are thus higher, semantically transparent cognitive states. citet:velleman89_pract_reflec, for example, may be categorized as a /strong/ cognitivist citep:sep-intention, as he holds that intentions are nothing but particular beliefs about actions. [[citet:velleman89_pract_reflec][p. 109]] states that intentions are ‘self-fulfilling expectations that are motivated by a desire for their fulfillment and that represent themselves as such’ [[citep:wilson16_action][p. 22]]. An argument in favor of this position is that it gives a ready explanatation of the claim that generally, an intentionally \phi-ing is accompanied by knowledge that one is \phi-ing, put forward by, citet:anscombe57_inten[fn::Anscombe adds to this that the knowledge is gained without observation, but I leave this rather controversial claim out of the picture for now. Interestingly, citet:sep-intention attributes an anti-cognitivist stance to Anscombe.] And since intentions already consist in beliefs, we'd just have to argue for their justification to ascribe knowledge. In some sense, then, this position is /reductive/: Intentions are reduced to mental states, and talk about intentions just is a useful shorthand for when we actually mean certain types of propositional attitudes. 

citet:Bratman1987-BRAIPA opposes cognitivism. He puts forward a couple of arguments, centering around the idea that intentions serve functions which aren't readily explained by appeal to beliefs and desires. One example of his arguments is that, roughly, intentions strongly motivate, while desires might be overridden, somewhat analogous to the philosophical distinction of /ultima facie/ reasons (intentions) and /prima facie/ (desires) reasons. This leaves something unexplained, which would be better served by explaining intentions as ``psychologically real and not reducible to desire-belief complexes'' [[citep:wilson16_action][p. 32]].  

citet:pacherie00_conten_inten argues against cognitivism and for intentions as irreducible, too. She also helpfully distinguishes between three types of intentions, future-directed (distal) intentions, present-directed (proximal) intentions and motor intentions. They descrease (in the listed order) in generality and complexity. Also, motor intentions tend to not share the same representability as a propositon. In Searle's terms, the distal intentions might correspond to prospective intentions, while the intention in action might share features of both proximal and motor intentions.

Of course, this is just a simplifying snapshot of some issues concerning intentions. There is, for example, a whole debate around whether you can \phi intentionally without intending to \phi (as, e.g., nananana citet:Bratman1987-BRAIPA notes). There is also no discussion of the rationality of intentions, or whether and how intentions relate to self-knowledge. There are all sorts of further complications and puzzles (as, for example, citet:pacherie_action_theor surveys) but only so little space in this paper.

This concludes the section of this paper on intention in the philosophical literature. Seemingly disparate, the next section discusses how perception and action are captured in a particular up-and-coming theory in the cognitive sciences, before I address how we can plausibly interface both approaches in the section after that.

* Predictive Processing

Predictive Processing is a term coined by Andy Clark addressing the question: /What/ do brains do (what is their function) citep:eliasmith07_how_to_build_brain. But what kind of framework is predictive processing? 
citet:marr82_vision offered to distinguish theories of information-processing, like the brain, into three different levels. The computational, the algorithmic and the implementational level, where implementation actually concerns the hardware, an algorithm describes, roughly, the /How/ and the computational or functional level is concerned with the more abstract /What/, /Why/ and /What for/.  
The framework can be applied to perception, cognition, and action and their interplay citep:wiese17_vanil_pp_philos. On Marr's taxonomy, the framework is chiefly employed on the computational and algorithmic level, but often with some idea as to neural implementation strategies or connection to empirical findings in neurophysiology.

What does predictive processing claim about the brain, then?
On the conventional picture of perception, the information flows from the world to sensory receptors through a hierarchy of neural feature detectors until reaching more abstract, cognitive facilities. The brain's role is that of a passive receptor, busily building up percepts out of raw sensory input. The higher up in the hierarchy, the more complex and abstract the contents. Predictive processing aspires to turn this conventional, passive, picture ``on its head'' \Su{51}, by developing that cognitive systems are ``constantly active, trying to predict [...] the streams of sensory stimulation before they arrive'' \Su{52}. The theory has two main components: By maintaining a complex, hierarchical, representation of the hidden causes of the world, the cognitive system constantly predicts the upcoming sensory input. What is fed forward in the brain, then, is not the raw sensory input, but instead the prediction errors as the mismatch of the prediction generated with the actual sensory input. If the predictions can be made more accurate than not, this is a strategy to compress the incoming data and significantly reducing the necessary information flow from the sensory organs. This compression strategy is called predictive coding and has more general application than in theories of perception. Its application in a cognitive system on all levels of a hierarchical model of the world constitutes, roughly, predictive processing as Clark proposes it. Key to this system is the ability of the brain to adapt its predictions by /minimizing prediction error/. 

Instead of the conventional description of perception as /bottom-up/, that is, from sensory organs to higher cognitive functions, perception on the predictive processing view describes a complex interplay of mostly /top-down/ predictions and /bottom-up/ reports of prediction errors. 

To see how this is supposed to work, let's look at the perception process in a bit more detail.

A central aspect is the hierarchical inner model which generates the system's predictions. Usually embedded in a constant, changing flow, let's freeze the frame, so to speak, for a second. The world as presented to a cognitive system consists in complex, nested hierarchical structures. To understand the causal relationship and leverage opportunities to interact with the world, the predictions generated need to be fairly accurate. The system needs to get a grip, as Clark calls it \Su{20}. A claim of the framework is that the organizational structure of the brain in some way mirrors the complex causal structures in form of hierarchies that encode different levels of information. 

A very short overview of how this process is meant to happen might be helpful. The levels of the hierarchy differ in how abstract---how removed from sensory input---the information is and operate at ``multiple spatial and temporal scales'' \Su{25}. Each level entertains a number of hypothesis about the lower level activity. If prediction is not successful, prediction error is generated and propagated up the hierarchy, where the predictions are adapted. This happens by selecting those hypotheses which minimize prediction error. `Selecting' stands for a complicated mixture of suppresion and selective enhancement \Su{37f.}, which regulates the /precision/ of generated prediction errors. Clark describes this process much more detail than there is room in this paper \Su{31f.}, but stays mostly on a conceptual level. Since prediction is inherently uncertain, it is a central part of the framework, however, that this process is approximating optimal Bayesian inference, that is, can be described by probabilistic models, in particular, hierarchical Bayesian models citep:orlanding_how_radic_predic_proces. From a computational standpoint, the hierarchical model is especially useful, since the structure encodes conditional independencies between parameters and thus simplifies calculations. In a sense, then, probabilistic dependencies are used to model causal dependencies in the world.[fn::Initially, I prepared a more detailed description of the principles of hierarchical Bayesian models but, to not obscure the central points, in the end ditched the rather technical exposition for the following examples.]

A single percept, say, a scene with a dog, is then represented across multiple levels of the hierarchy, with lower levels trying to account for simpler structures, like color patches, edges, etc., with medium levels concerned with more complex structures like, say, the dog, while higher level could represent more complex matters still and enable the percept of the whole scene. Clark coins the term ``multilevel cascade'' for the whole process, as predictions `cascade' down the hierarchy.

Clark gives an example which helps to flesh out the overall picture:

\begin{quote}
Thus imagine you are kidnapped, blindfold, and taken to some unknown location. As the blindfolds are removed, your brain's first attempts at predicting the scene will surely fail. But rapidly processed, low spatial frequency cues soon get the predictive brain into the right general ballpark. Framed by these earle emerging gist elements [...] subsequent processing can be guided by specific mismatches with early attempts to fill the scene. These allow the system to progressively tune its top-down predictions, until it settles on a coherent overall interpretation pinning down details at many scales of space and time. \Su{42}
\end{quote}

Clark takes a radical citep:orlanding_how_radic_predic_proces turn from the passive type of perceptual (subpersonal) inference just described to what he and citet:friston11_action_under_activ_infer call /active inference/. This is the additional claim that action, too, can be explained with hierarchical models and minimization of prediction error. That is, the same general principles apply to both action and motor control as to cognition and perception. Instead of two conceptually and locally different processes, the framework aspires to explain the functions with one fell swoop. 

What is that supposed to mean? This means there are two ways in which prediction error can be minimized: ``either the system can update the parameters of its inner models, in order to generate new predictions about what is causing the incoming sensory data (perceptual inference), or it can keep its generative model fixed, and resample the world such that the incoming sensory data accords with the predictions (active inference)'' citep:burr17_embod_decis_predic_brain. The brain employs the ``twin strategies of altering predictions to fit the world, and altering the world to fit predictions`` \Su{122}. That is, I move my arm down to minimize the dissonance between my prediction that my arm is down and the sensory input which reports it still up. At first glance, this runs counter to intuition. I certainly do not consciously predict my arm to be in that position, that sounds like hallucinating.  What Clark proposes is that predictions responsible for motor commands are /subjunctive/ or counterfactual. They resemble imagination or wishful thinking. By increasing precision on a hypothesis, the system can also increase its urgency: the counterfactual hypothesis is more likely to be enacted. First and foremost, the predictions relevant to motor control tend to concentrate on proprioception (sensory feedback on the position on one's limbs) instead of other, external sensory cues \Su{122}, which makes the competition with perceptual prediction less pressing. Action happens after a corresponding expectation has been selected by the system. In some sense, then, these expectations are `self-fulfilling prophecies' citep:clark15_predic_peace. What is more, according to Clark, the correct interpretation of the type of hypothesis the different levels entertain are not some kind of ``action-neutral image of an objective realm'', but instead ``possibilities for action and intervention that the environment makes available to the agent'', so-called /affordances/ \Su{171}. Although, here comes the caveat, it is harder to extend this embodied story into ever more abstract territory like long-term planning, language or mathematical reasoning citep:klein18_what_do_predic_coder_want.

Before evaluating where intentions might fit into this picture, let's address what speaks in favor of accepting the predictive processing framework. Since this is not at all the focus of this paper, I'll keep it short. It certainly speaks in the frameworks favor that it shows promise of a wide, unifying approach, building bridges between empirical and theoretical work on cognition citep:wiese17_vanil_pp_philos. Additionally, the range and explanatory power of models in the framework is impressive. It explains peculiar empirical observations like binocular rivalry \Su{33} and a host of optical illusions (citet:hohwy13_predic_mind seems to be particularly excited about those). Moreover, the framework---by its hierarchical structure---inherits the explanatory power of more general hierarchical Bayesian models \Su{173}, which have been remarkably successful at accounting for a whole range of learning scenarios (citet:tenenbaum11_how_to_grow_mind,perfors11_tutor_introd_to_bayes_model_cognit_devel give a compelling overview). But a framework can be explanatory powerful but still be completely useless to science if theories posed in it do not make verifiable empirical claims at all. This is a contentious point, and some claim that the framework teeters on the verge of triviality citep:sims17_probl_predic, whereas Clark naturally sees that differently. He constantly stresses the abundance of non-philosophical results which can be interpreted in favor of predictive processing. For example, the artificial neural implementation provided by citet:rao99_predic_codin_visual_cortex gives a kind of `proof-of-concept' of many principles involved. Other empirical studies can validate some quite detailed conjectures, such as about neural response dynamics citep:jehee09_predic_feedb_can_accoun_biphas, seem to offer some confirmation. This is a research issue in its own right, not only because there are competing theories of motor control using much of the same formalisms citep:rescorla17_book_review_surfin_uncer_by_andy_clark,orlanding_how_radic_predic_proces,jehee09_predic_feedb_can_accoun_biphas. 

So far for the exposition of the predictive processing framework. In the following section, I argue for ways in which we might ascribe intentions to an agent in the framework, and give an account of which problems lurk under the superficial similarities.

* Intentions in Predictive Processing
The pressing question, now, of course: is there a place for intentions in the predictive processing framework? Since this paper does not argue for a clear-cut, but only qualified answer, I will proceed by discussing where bringing both together fits well and where its still lacking.

citet:rescorla17_book_review_surfin_uncer_by_andy_clark argues that there is not much promise: If at all, the predictive processor might adopt an eliminativist (that is, getting rid of intention-talk altogether, which he ascribes to Friston) or a reductivist stance (which he ascribes to Clark), but it is far from clear. Let's analyze this more closely.

As a first common ground, Anscombe's important distinction of direction of fit as described above does find its counterpart in the predictive processing framework. On the one hand there is a clear mind-to-world direction of fit when updating the internal model to fit the sensory input. On the other hand, issuing motor commands in order to bring the sensory input in accordance with the predicted input reveals a world-to-mind direction of fit. Both ways of reducing prediction error capture an aspect of the folk psychological point Anscombe makes. The mechanism or underlying principle remains the same in both cases \Su{123}. In a certain sense, then, establishing predictions can be seen as belief-like or as desire-like, depending on how the prediction error was minimized. [[citet:friston11_action_under_activ_infer][p. 138]] even state that the ``conventional distinction between sensory (consequence) and motor (cause) representations'' is /destroyed/. For them, the desire-like state already coincides with intention, it forms an ``intent''. This stands in some contrast to an eliminativist position mentioned above. 

Could it be that simple, have we found the intentions of philosophical discourse already? To begin with, Bratman's argument against the cognitivist position rested on differences between desires and intentions. Intentions go beyond desires, include a decision; they end deliberation. On Friston's picture above, it might seem as though both concepts are equivalent. But this judgment neglects the intricacies of the predictive coding mechanism. A lot of attention is paid to working out the way in which precision modulation interacts with prediction errors [[cite:clark16_surfin_uncer][Ch. 2]]. Suppose the agent considers different future hypotheses concerning the movement of their own body. The system can increase the precision on different counterfactual hypotheses. In this stadium, they are desire-like: There are potentially multiple such hypotheses representing different affordances. But as of yet, none has definitely been chosen. Deliberation is still not over. Once the gain on a hypothesis is high enough, it `wins' over and actually generates motor commands cite:burr17_embod_decis_predic_brain. At this point, a decision has been made, so to speak. This account fits better with Bratman's distinction. 

The problem of the deviant causal chain can be straightforwardly addressed in the framework. citet:friston14_anatom_choic introduce the concept of a control sequence, i.e. sensory expectations associated with a sequence of descending proprioceptive predictions [[cite:burr17_embod_decis_predic_brain][p. 9]], which guides the movement through its expected stages. In the case of the deviant chain, one or more of the intermediate stages of the sequence are frustrated by high prediction error, removing the causal support for the movement. Relating this control sequence---a higher level expectation---to intention in action as in Searle's picture, his philosophical argument against causal chains can be leveraged here, too. 

But two additional issues seriously dim the prospect of a simple integration. First, intentions are strongly tied to what the agent wants and what motivates them. It is not at all clear, however, how that is addressed in the framework. Consider my expectation that my arm is not in the position my sensory input reports. Why would I not instead of moving it there update my predictions of where my arm is? This seems much more economically reasonable. In fact, this issue is a general one and brought forth so often that it got a name, the /dark room problem/ citep:sims17_probl_predic,klein18_what_do_predic_coder_want. It goes like this: If prediction error minimization is the central driving force of an agent, why do they not sit in a dark room, where they are ``correctly predicting immobility and darkness until all bodily functions cease'' \Su{47}? What motivates the agent to go out? citet:hohwy13_predic_mind argues that desire-like states generally have higher precision-gain than more descriptive states and are therefore likelier to be chosen. Clark seems to favor a similar route, focusing more on interoceptive-focused predictions (concerning hunger, thirst etc. [[citep:clark16_surfin_uncer][Ch. 6]]).[fn::I haven't reconstructed his answer for the purposes of this paper yet. Note to self for future research!] According to citet:klein18_what_do_predic_coder_want, this still does not explain why a particular action is executed instead of another action of similar type, which for him is only resolvable by appeal to certain innate desires or preferences. That, however, does not seem to be a big problem. Quite plausibly, there are /some/ innate belief-like states.

The second issue arises when looking at Pacherie's distinction of intentions. What Clark and Friston describe as intentions foremost resembles motor intentions in her terminology: These run rather automatic and are normally unconscious. Similarly, the processes of perception and simple motor control does seem to be unperturbed by higher cognitive functions (barring phenomenons like cognitive penetration for the moment). We might stretch the concepts of predictive processing to proximal intentions, similar to the description of intention in action above. What is more difficult, however, is the modeling of distal intention that involve copious amounts of abstract planning. One could argue that these are best represented by appeal to propositional attitudes, as in I plan to finish this paper this evening before the deadline, what some might call folk psychological discourse citep:pylyshyn86_comput_cognit. But since the representation in the predictive processing framework are inherently probabilistic and hardly symbolic, there is as of yet[fn::Meaning I did not find as of yet.] no clear path from the representations of sensory input of the bottom level to complex future-involving goal representation. But perhaps we do not need to pinpoint a clear computational process going on in the brain that corresponds to complex propositional attitudes, anyway, and instead can be satisfied with descriptions of systems that are functionally equivalent, in the sense that the emergent behavior of the system warrants the ascription of a propositional attitude to the agent, or along these lines argues cite:dewhurst17_folk_psych_bayes_brain.

* Conclusion 

How can this analysis be judged, in summary? I think the initially quite obvious move is still attractive: Velleman's strong cognitivism is almost verbatim applicable to the preditive processing framework. Recall, he stated that intentions are ‘self-fulfilling expectations that are motivated by a desire for their fulfillment and that represent themselves as such’. Since the predictive mind is all about expectations, in a loose sense, the strong cognitivist seems like the first choice here. But only, really, if you aren't very particular about it. The cognitivist position is associated with intentions as propositional attitudes, and the complications of that have been described in the previous section. Apart from that, I think that intentions have a plausible place in the framework as those hypotheses which have been selected by precision modulation. This does not imply a cognitivist position, however. What it /does/ enforce is a commitment to intentions as mental states, which might come as no surprise with a bit og knowledge about the framework. 

Looking forward, I think there is a lot of potential of mediating between folk psychology in general, findings in philosophical discourse and models in cognitive science, as they can fruitfully inform each other. More concretely, I haven't touched the question of the rationality of intentions at all. But since the predictive processing framework deals in probabilities, there might certainly be an angle to address this issue, too. I also simplified much of the discussion and left out quite a bit of nuances in what makes an intention, where one could continue this research. 

\printbibliography{}
